HAMLET is a lightweight, all-in-one framework for building and iterating on LLM agents. It lets you define agents, launch a Gradio GUI, instrument runs with Langfuse, and fine-tune models using the built-in GRPO trainer. The framework is modified primarily on two open-source projects: [smolagents](https://github.com/huggingface/smolagents) for the core agent structure (`src/hamlet/core`) and [verifiers](https://github.com/PrimeIntellect-ai/verifiers) for the training stack (`src/hamlet/train`).

## Installation

Follow the steps below to set up HAMLET in a fresh environment. The project uses [uv](https://github.com/astral-sh/uv) to manage Python dependencies because it keeps lock files fast and reproducible. You can still use `pip`, but uv is the recommended path.

### 1. Prerequisites
- Python 3.10 or newer (3.11+ works as well)
- uv (install with the command below or follow the instructions in the uv repository)

```powershell
pip install uv
```

### 2. Clone the repository
```powershell
git clone https://github.com/MINDS-THU/HAMLET.git
cd HAMLET
```

### 3. Install dependencies
Install the base runtime:
```powershell
uv sync
```

Optional extras:
- `uv sync --extra tools` for the toolchain utilities (file editing, retrieval, visual QA, etc.).
- `uv sync --extra train` for the training stack (GRPO trainer, vLLM client, etc.).

### 4. Use the environment
- Run commands inside the uv-managed env with `uv run`, e.g. `uv run pytest` or `uv run python examples\gradio_gui_example.py`.
- Alternatively, activate the virtual environment directly: `.\.venv\Scripts\activate` on Windows or `source .venv/bin/activate` on Unix shells.

### 5. Configure API keys
Create a `.env` file (or export environment variables) with the credentials your agent or tools need. Common entries include `OPENAI_API_KEY`, `OPENAI_BASE_URL`, and any service-specific tokens (HuggingFace, Langfuse, etc.). The examples load this file via `dotenv`, so keep it at the repo root and never commit it.

## Getting Started

All commands below assume you are in the repository root after running `uv sync`.

### 1. Gradio GUI walkthrough
```powershell
uv run python examples\gradio_gui_example.py
```
Launches the interactive UI so you can chat with an agent, observe tool traces, and any artifact generated by the agent.
![GUI_Example](./resource/GUI_example.png)

### 2. Parallel code blocks
```powershell
uv run python examples\parallel_code_blocks_example.py
```
Shows how HAMLET dispatches multiple LLM-generated code blocks concurrently and uses an optional `Early Stop Strategy: code` snippet to stop remaining executions once a shared goal is met.

Example run (summarized):
```
Thought:
- Strategy 1: closed-form sum of squares n(n+1)(2n+1)/6
- Strategy 2: direct iteration sum(i * i for i in range(1, 76))
- Early Stop Strategy: code ensures 0 <= total <= 500000

Code#1 and Code#2 execute in parallel, both printing their totals.
Early stop code runs afterward and cancels the remaining execution once the bound check passes.
```

Log excerpt:
```
─ Concurrently executing parsed Code#1 (2 in total)
Method: formula, total: 143450
─ Concurrently executing parsed Code#2 (2 in total)
Method: iterative sum, total: 143450
Execution result of Code#1 passed, other code executions will be cancelled.
```

### 3. Structured schema I/O
```powershell
uv run python examples\structured_schema_example.py
```
Demonstrates attaching JSON schemas to agent inputs and outputs so downstream automation receives validated, typed payloads. The sample defines:

```python
class PersonaRequest(BaseModel):
	persona: str
	goal: str
	tone: str

class PersonaResponse(BaseModel):
	summary: str
	talking_points: list[str]
	next_steps: list[str]
```

HAMLET hands these schemas to the LLM, validates the JSON that comes back, and surfaces parsing errors if the reply drifts from the contract.

Tip: prepend `uv run` to each script while uv manages the environment, or activate `.venv` if you prefer calling `python` directly.
