#!/usr/bin/env python
# coding=utf-8

import os
import json
import tempfile
import threading
from typing import Dict, List
from pathlib import Path

#!/usr/bin/env python
# coding=utf-8

import os
import re
from typing import Optional

from hamlet.core.agent_types import AgentAudio, AgentImage, AgentText
from hamlet.core.agents import MultiStepAgent, PlanningStep
from hamlet.core.memory import ActionStep, FinalAnswerStep, MemoryStep
from hamlet.core.utils import _is_package_available
from hamlet.core.models import ChatMessageStreamDelta


def get_step_footnote_content(step_log: MemoryStep, step_name: str) -> str:
    """Get a footnote string for a step log with duration and token information"""
    step_footnote = f"**{step_name}**"
    if hasattr(step_log, "input_token_count") and hasattr(step_log, "output_token_count"):
        token_str = f" | Input tokens:{step_log.input_token_count:,} | Output tokens: {step_log.output_token_count:,}"
        step_footnote += token_str
    if hasattr(step_log, "duration"):
        step_duration = f" | Duration: {round(float(step_log.duration), 2)}" if step_log.duration else None
        step_footnote += step_duration
    step_footnote_content = f"""<span style="color: #bbbbc2; font-size: 12px;">{step_footnote}</span> """
    return step_footnote_content


def pull_messages_from_step(step_log: MemoryStep, skip_model_outputs: bool = False):
    """Extract ChatMessage objects from agent steps with proper nesting"""
    if not _is_package_available("gradio"):
        raise ModuleNotFoundError(
            "Please install 'gradio' extra to use the GradioUI: `pip install 'smolagents[gradio]'`"
        )
    import gradio as gr

    if isinstance(step_log, ActionStep):
        # Output the step number
        step_number = f"Step {step_log.step_number}" if step_log.step_number is not None else "Step"
        if not skip_model_outputs:
            yield gr.ChatMessage(role="assistant", content=f"**{step_number}**")

        # First yield the thought/reasoning from the LLM
        if (not skip_model_outputs) and hasattr(step_log, "model_output") and step_log.model_output is not None:
            # Clean up the LLM output
            model_output = step_log.model_output.strip()
            # Remove any trailing <end_code> and extra backticks, handling multiple possible formats
            model_output = re.sub(r"```\s*<end_code>", "```", model_output)
            model_output = re.sub(r"<end_code>\s*```", "```", model_output)
            model_output = re.sub(r"```\s*\n\s*<end_code>", "```", model_output)
            model_output = model_output.strip()
            yield gr.ChatMessage(role="assistant", content=model_output)

        # For tool calls, create a parent message
        if hasattr(step_log, "tool_calls") and step_log.tool_calls is not None:
            first_tool_call = step_log.tool_calls[0]
            used_code = first_tool_call.name == "python_interpreter"
            parent_id = f"call_{len(step_log.tool_calls)}"

            # Tool call becomes the parent message with timing info
            args = first_tool_call.arguments
            if isinstance(args, dict):
                content = str(args.get("answer", str(args)))
            else:
                content = str(args).strip()

            if used_code:
                # Clean up the content by removing any end code tags
                content = re.sub(r"```.*?\n", "", content)
                content = re.sub(r"\s*<end_code>\s*", "", content)
                content = content.strip()
                if not content.startswith("```python"):
                    content = f"```python\n{content}\n```"

            parent_message_tool = gr.ChatMessage(
                role="assistant",
                content=content,
                metadata={
                    "title": f"ğŸ› ï¸ Used tool {first_tool_call.name}",
                    "id": parent_id,
                    "status": "done",
                },
            )
            yield parent_message_tool

        # Display execution logs if they exist
        if hasattr(step_log, "observations") and (
            step_log.observations is not None and step_log.observations.strip()
        ):
            log_content = step_log.observations.strip()
            if log_content:
                log_content = re.sub(r"^Execution logs:\s*", "", log_content)
                yield gr.ChatMessage(
                    role="assistant",
                    content=f"```bash\n{log_content}\n",
                    metadata={"title": "ğŸ“ Execution Logs", "status": "done"},
                )

        # Display any errors
        if hasattr(step_log, "error") and step_log.error is not None:
            yield gr.ChatMessage(
                role="assistant",
                content=str(step_log.error),
                metadata={"title": "ğŸ’¥ Error", "status": "done"},
            )

        # Update parent message metadata to done status without yielding a new message
        if getattr(step_log, "observations_images", []):
            for image in step_log.observations_images:
                path_image = AgentImage(image).to_string()
                yield gr.ChatMessage(
                    role="assistant",
                    content={"path": path_image, "mime_type": f"image/{path_image.split('.')[-1]}"},
                    metadata={"title": "ğŸ–¼ï¸ Output Image", "status": "done"},
                )

        # Handle standalone errors but not from tool calls
        if hasattr(step_log, "error") and step_log.error is not None:
            yield gr.ChatMessage(role="assistant", content=str(step_log.error), metadata={"title": "ğŸ’¥ Error"})

        yield gr.ChatMessage(role="assistant", content=get_step_footnote_content(step_log, step_number))
        yield gr.ChatMessage(role="assistant", content="-----", metadata={"status": "done"})

    elif isinstance(step_log, PlanningStep):
        if not skip_model_outputs:
            yield gr.ChatMessage(role="assistant", content="**Planning step**")
            yield gr.ChatMessage(role="assistant", content=step_log.plan)
            yield gr.ChatMessage(role="assistant", content=get_step_footnote_content(step_log, "Planning step"))
            yield gr.ChatMessage(role="assistant", content="-----", metadata={"status": "done"})

    elif isinstance(step_log, FinalAnswerStep):
        final_answer = step_log.final_answer
        if isinstance(final_answer, AgentText):
            yield gr.ChatMessage(
                role="assistant",
                content=f"**Final answer:**\n{final_answer.to_string()}\n",
            )
        elif isinstance(final_answer, AgentImage):
            yield gr.ChatMessage(
                role="assistant",
                content={"path": final_answer.to_string(), "mime_type": "image/png"},
            )
        elif isinstance(final_answer, AgentAudio):
            yield gr.ChatMessage(
                role="assistant",
                content={"path": final_answer.to_string(), "mime_type": "audio/wav"},
            )
        else:
            yield gr.ChatMessage(role="assistant", content=f"**Final answer:** {str(final_answer)}")

    else:
        raise ValueError(f"Unsupported step type: {type(step_log)}")


def stream_to_gradio(
    agent,
    task: str,
    task_images: list | None = None,
    reset_agent_memory: bool = False,
    additional_args: dict | None = None,
):
    """Runs an agent with the given task and streams the messages as gradio ChatMessages."""
    if not _is_package_available("gradio"):
        raise ModuleNotFoundError("Install with: pip install 'smolagents[gradio]'")

    import gradio as gr

    intermediate_text = ""
    for step_log in agent.run(
        task, images=task_images, stream=True, reset=reset_agent_memory, additional_args=additional_args
    ):
        # copy token counts onto the step, if they exist
        if getattr(agent.model, "last_input_token_count", None) is not None and isinstance(
            step_log, (ActionStep, PlanningStep)
        ):
            step_log.input_token_count = agent.model.last_input_token_count
            step_log.output_token_count = agent.model.last_output_token_count

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Memory steps â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if isinstance(step_log, MemoryStep):
            intermediate_text = ""  # reset buffer
            for msg in pull_messages_from_step(
                step_log,
                skip_model_outputs=getattr(agent, "stream_outputs", False),
            ):
                yield msg

        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Streaming deltas â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        elif isinstance(step_log, ChatMessageStreamDelta):
            intermediate_text += step_log.content or ""
            yield intermediate_text


class AgentConfigManager:
    """Class for managing agent configurations"""
    
    def __init__(self, config_dir: str = "agent_configs"):
        self.config_dir = Path(config_dir)
        self.config_dir.mkdir(exist_ok=True)
        # Default (global) config file; may be overridden per user
        self.agent_configs_file = self.config_dir / "agent_configs.json"
        self._user_id = None
        # Inâ€‘process lock to avoid concurrent writers clobbering temp file
        self._lock = threading.Lock()
        # Note: We no longer use tool_configs.json since we have dynamic discovery
        
        # Initialize configuration files
        self._init_config_files()

    # ---------------- Per-User Support ----------------
    def set_user(self, user_id: str):
        """Set the active user so configs are isolated per student.

        A sanitized and hashed filename is created to avoid exposing raw identifiers
        in directory listings while still being deterministic for returning students.
        """
        import re, hashlib
        if not user_id or not user_id.strip():
            raise ValueError("User ID cannot be empty.")
        raw = user_id.strip()
        # Sanitize (allow alnum, dash, underscore)
        safe = re.sub(r'[^a-zA-Z0-9_-]', '_', raw)
        if not safe:
            safe = 'user'
        # Short hash to avoid collisions / leaking full ID
        short_hash = hashlib.sha256(raw.encode('utf-8')).hexdigest()[:16]
        self._user_id = safe
        self.agent_configs_file = self.config_dir / f"{safe}_{short_hash}_agent_configs.json"
        if not self.agent_configs_file.exists():
            # Initialize with defaults for brand-new user
            self._init_config_files(force=True)
        else:
            # Do nothing; existing file will be used
            pass

    def get_user(self) -> Optional[str]:  # type: ignore[name-defined]
        return self._user_id
    
    def _init_config_files(self, force: bool = False):
        """Initialize configuration files with default values"""
        if force or not self.agent_configs_file.exists():
            self.save_agent_configs(self._build_example_seed())
        
        # Note: We no longer create tool_configs.json as we use dynamic discovery
    
    def get_all_agent_metadata(self) -> Dict[str, Dict]:
        """Get all agent configurations"""
        if self.agent_configs_file.exists():
            try:
                with open(self.agent_configs_file, 'r', encoding='utf-8') as f:
                    data = f.read().strip()
                    if not data:
                        raise ValueError("empty config file")
                    return json.loads(data)
            except Exception as e:
                print(f"[AgentConfigManager] Failed to read config '{self.agent_configs_file}' ({e}); attempting recovery.")
                self._init_config_files(force=True)
                try:
                    with open(self.agent_configs_file, 'r', encoding='utf-8') as f2:
                        return json.load(f2)
                except Exception:
                    return {}
        return {}
    
    def get_all_tool_metadata(self) -> List[str]:
        """Get all tool names using dynamic discovery"""
        try:
            # Import here to avoid circular imports
            from default_tools import get_available_tools
            return get_available_tools()
        except ImportError:
            # Fallback if import fails
            return []
    
    def save_agent_configs(self, configs: Dict[str, Dict]):
        """Save agent configurations"""
        # Use unique temp file + lock to prevent race where two writers move the same temp file.
        with self._lock:
            # Ensure target directory still exists (may have been removed externally).
            self.config_dir.mkdir(exist_ok=True)
            fd, tmp_path = tempfile.mkstemp(
                dir=str(self.config_dir),
                prefix=self.agent_configs_file.stem + "_",
                suffix=".tmp"
            )
            tmp_file = Path(tmp_path)
            try:
                with os.fdopen(fd, 'w', encoding='utf-8') as f:
                    json.dump(configs, f, indent=2, ensure_ascii=False)
                try:
                    os.replace(tmp_file, self.agent_configs_file)
                except FileNotFoundError:
                    # Rare case: tmp file vanished (external cleanup) OR directory removed.
                    # Fall back to a direct write.
                    with open(self.agent_configs_file, 'w', encoding='utf-8') as f:
                        json.dump(configs, f, indent=2, ensure_ascii=False)
                except Exception as e:
                    # As a fallback to preserve user data, attempt direct write.
                    with open(self.agent_configs_file, 'w', encoding='utf-8') as f:
                        json.dump(configs, f, indent=2, ensure_ascii=False)
                    print(f"[AgentConfigManager] Warning: atomic replace failed ({e}); used direct write.")
            finally:
                if tmp_file.exists():
                    try:
                        tmp_file.unlink()
                    except Exception:
                        pass
    
    def add_agent_config(self, name: str, config: Dict):
        """Add new agent configuration"""
        configs = self.get_all_agent_metadata()
        configs[name] = config
        self.save_agent_configs(configs) 
    
    def _build_example_seed(self) -> Dict[str, Dict]:
        """æ„å»ºæ–‡çŒ®æœç´¢æ™ºèƒ½ä½“ï¼ˆç”¨äºé¦–æ¬¡åˆå§‹åŒ–æˆ–æŸåæ¢å¤ï¼‰ã€‚"""
        example_name = "æ–‡çŒ®æœç´¢æ™ºèƒ½ä½“"
        tools = [
            "web_search",                 # æ£€ç´¢
            "get_paper_from_url",         # æ‰¹é‡æŠ“å–è®ºæ–‡å…¨æ–‡
            "create_file_with_content",   # åˆæ¬¡å†™å…¥ Markdown
            "see_text_file",              # æŸ¥çœ‹æ–‡ä»¶å†…å®¹
            "modify_file",                # å±€éƒ¨ä¿®æ”¹
            "list_dir",                   # ç›®å½• / å»é‡ / é¿å…è¦†ç›–
            "delete_file_or_folder"       # ä»…åœ¨ç”¨æˆ·æ˜ç¡®è¦æ±‚åˆ é™¤æ—¶ä½¿ç”¨
        ]
        description = (
            "æ ¹æ®ç”¨æˆ·æä¾›çš„ç ”ç©¶ä¸»é¢˜è‡ªåŠ¨æ£€ç´¢ç›¸å…³è®ºæ–‡ï¼Œä¸‹è½½æ­£æ–‡ï¼ŒæŠ½å–ä¸æ•´ç†å…³é”®ä¿¡æ¯ï¼Œ"
            "å¹¶ç”ŸæˆåŒ…å«æ¯ç¯‡è®ºæ–‡æ ‡é¢˜ã€åŸå§‹é“¾æ¥åŠç»“æ„åŒ–æ€»ç»“çš„æŠ¥å‘Š (literature_report.md)ã€‚"
            "æ”¯æŒå¢é‡è¿½åŠ ã€ä¸»é¢˜æ¾„æ¸…ä¸æ–‡ä»¶åŒ–å­˜æ¡£ã€‚"
        )
        prompt = (
            "ä½ æ˜¯ä¸€åé¢å‘ç§‘ç ”äººå‘˜çš„â€œæ–‡çŒ®æœç´¢ä¸ç»¼è¿°åŠ©æ‰‹â€ã€‚ä½ çš„æ ¸å¿ƒä»»åŠ¡ï¼šæ ¹æ®ç”¨æˆ·ç»™å‡ºçš„ç ”ç©¶ä¸»é¢˜æˆ–é—®é¢˜ï¼Œè‡ªåŠ¨æ£€ç´¢ã€è·å–ã€æ•´ç†å¹¶ç”Ÿæˆç»“æ„åŒ–çš„å¤šç¯‡è®ºæ–‡ç»¼è¿°æŠ¥å‘Šã€‚\n\n"
            "ã€æ€»ä½“æµç¨‹ã€‘\n"
            "1. ä»»åŠ¡ç†è§£ä¸æ¾„æ¸…ï¼šä¸»é¢˜è¿‡å®½æˆ–å«ç³Šå…ˆæå‡º 1~2 ä¸ªæ¾„æ¸…é—®é¢˜ï¼ˆé¢†åŸŸ / å¹´é™ / æœŸæœ›ç¯‡æ•°ï¼‰ã€‚é»˜è®¤ 5 ç¯‡ã€‚\n"
            "2. åˆæ­¥æ£€ç´¢ (web_search)ï¼šç”Ÿæˆ 2~3 ç»„å¤šæ ·åŒ–æŸ¥è¯¢ï¼ˆåŒä¹‰è¯/æ–¹æ³•/åœºæ™¯ï¼‰ï¼›å»é‡ç­›é€‰å­¦æœ¯æ¥æºã€‚\n"
            "3. è®ºæ–‡è·å– (get_paper_from_url)ï¼šæ”¶é›†å‰ N æ¡å€™é€‰ PDF/é¡µé¢ï¼Œä¸€æ¬¡æ‰¹é‡æŠ“å–ï¼›å¤±è´¥è®°å½•ä½†ä¸é˜»å¡æ•´ä½“ã€‚\n"
            "4. æœ¬åœ°æ–‡ä»¶ï¼šä¸ºæ¯ç¯‡åˆ›å»º papers/<slug>.mdï¼Œç»“æ„å«ï¼šæ ‡é¢˜ã€Sourceã€æ‘˜è¦æ¸…æ´—ã€ç ”ç©¶é—®é¢˜ã€æ•°æ®/åœºæ™¯ã€æ–¹æ³•è¦ç‚¹ã€ä¸»è¦ç»“æœ(å®šé‡ä¼˜å…ˆ)ã€ä¼˜åŠ¿ä¸åˆ›æ–°ã€å±€é™æ€§ã€ä¸ä¸»é¢˜ç›¸å…³æ€§ã€‚\n"
            "5. ç”Ÿæˆ literature_report.mdï¼šç›®å½• + æ¯ç¯‡ 100~180 å­—ç²¾ç‚¼æ€»ç»“ + ä¸€å¥è¯å¯ç¤ºã€‚\n"
            "6. å¯¹è¯è¾“å‡ºï¼šç®€è¦é¢„è§ˆç»Ÿè®¡ä¸åç»­å¯é€‰æ“ä½œï¼ˆè¿½åŠ æ£€ç´¢ / ç¼©å°èŒƒå›´ / ç”Ÿæˆå¯¹æ¯”è¡¨ï¼‰ã€‚\n\n"
            "ã€å·¥å…·ä½¿ç”¨å‡†åˆ™ã€‘\n"
            "- web_searchï¼šè‹¥ç»“æœä¸è¶³æˆ–ç”¨æˆ·è¦æ±‚è¿½åŠ å†è°ƒç”¨ï¼›é¿å…æ— èŠ‚åˆ¶å¾ªç¯ã€‚\n"
            "- get_paper_from_urlï¼šæ‰¹é‡ä¸€æ¬¡ï¼›å¤±è´¥ä¸å¤šäº 1 æ¬¡é‡è¯•ã€‚\n"
            "- create_file_with_contentï¼šé¦–æ¬¡å†™æ–‡ä»¶ï¼›æ”¹åŠ¨ç”¨ modify_fileã€‚\n"
            "- modify_fileï¼šä»…æ”¹æœ€å°å¿…è¦è¡Œï¼›å‰ç½® see_text_file å®šä½ã€‚\n"
            "- list_dirï¼šé˜²æ­¢è¦†ç›– / å»é‡ / ç»Ÿè®¡ã€‚\n"
            "- delete_file_or_folderï¼šé™¤éç”¨æˆ·æ˜ç¡®è¯´æ˜åˆ é™¤å¯¹è±¡ã€‚\n\n"
            "ã€è´¨é‡ä¸å®‰å…¨ã€‘\n"
            "- ä¸æœæ’°æœªå‡ºç°çš„æ•°æ®ï¼›ç¼ºå¤±å†™â€œ(åŸæ–‡æœªæä¾›)â€ã€‚\n"
            "- æ‘˜è¦ä¸è¶³æ—¶å¯ç”¨æ­£æ–‡å¼€å¤´è¡¥é½å¹¶æ ‡æ³¨â€œ(è¡¥å……)â€ã€‚\n"
            "- ä¸­æ–‡æ€»ç»“ï¼›è‹±æ–‡æ ‡é¢˜å¯ä¿ç•™ã€‚\n"
            "- ç›¸ä¼¼æ–¹æ³•å¤šç¯‡å‡ºç°æ—¶åœ¨æŠ¥å‘Šä¸­åšç®€æ´å¯¹æ¯”ã€‚\n"
            "- æ§åˆ¶ä¸‹è½½æ•°é‡ï¼šè‹¥å€™é€‰ > éœ€æ±‚ 2 å€å…ˆæœ¬åœ°ç­›é€‰å†æŠ“å–ã€‚\n\n"
            "ã€å›ç­”æ ¼å¼ï¼ˆå¯¹è¯ï¼‰ã€‘\n"
            "- éœ€æ¾„æ¸…ï¼šåˆ—å‡ºæ¾„æ¸…ç‚¹å¹¶ç­‰å¾…ç”¨æˆ·ã€‚\n"
            "- å·²å®Œæˆæ£€ç´¢æŠ“å–ï¼šè¯´æ˜æˆåŠŸç¯‡æ•°ã€æŠ¥å‘Šæ–‡ä»¶åç§°ã€åç»­å¯é€‰æ“ä½œèœå•ã€‚\n"
            "- éƒ¨åˆ†å¤±è´¥ï¼šåˆ—å‡ºå¤±è´¥ URL + åŸå› æ‘˜è¦ã€‚\n\n"
            "ç°åœ¨ç­‰å¾…ç”¨æˆ·è¾“å…¥çš„ç ”ç©¶ä¸»é¢˜ã€‚"
        )
        return {example_name: {"prompt": prompt, "description": description, "tools": tools, "sub_agents": [], "agent_type": "CodeAgent"}}